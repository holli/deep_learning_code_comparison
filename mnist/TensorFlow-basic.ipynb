{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/tf_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/tf_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/tf_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/tf_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_input_data = input_data.read_data_sets('./data/tf_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len 55000, test_len 10000\n"
     ]
    }
   ],
   "source": [
    "mnist_train = mnist_input_data.train\n",
    "mnist_test = mnist_input_data.test\n",
    "print(\"train_len {}, test_len {}\".format(mnist_train.num_examples, mnist_test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (784,)\n",
      "Sample target: [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e5ace7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See one example\n",
    "print(\"Sample shape: {}\".format(mnist_train.images[0].shape))\n",
    "print(\"Sample target: {}\".format(mnist_train.labels[0]))\n",
    "sample_img = mnist_train.images[0]\n",
    "sample_img = sample_img.reshape(28,28)\n",
    "# print(\"Sample size: {}\".format(mnist_train.images[0].size()))\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(sample_img).set_cmap('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are two different network definitions. Half of the tutorials seem to do everything manually and half use tf.layers so they are both shown here. There are no big functional differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw model by manually defining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn_raw(x_ph):\n",
    "    x = tf.reshape(x_ph, [-1, 28, 28, 1])\n",
    "\n",
    "    with tf.name_scope('conv1'):\n",
    "        weights = tf.Variable(tf.random_normal([5, 5, 1, 32]))\n",
    "        biases = tf.Variable(tf.random_normal([32]))\n",
    "        x = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        weights = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "        biases = tf.Variable(tf.random_normal([64]))\n",
    "        x = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.name_scope('dense1'):\n",
    "        weights = tf.Variable(tf.random_normal([7*7*64, 1024]))\n",
    "        biases = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "        x = tf.reshape(x, [-1, weights.get_shape().as_list()[0]])\n",
    "        x = tf.add(tf.matmul(x, weights), biases)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # This would be one way to use dropout. In sess.run we would have to add\n",
    "        # {dropout_keep_prob: 0.5} to feed_dict while training\n",
    "        # dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        # x = tf.nn.dropout(x, dropout_keep_prob)\n",
    "\n",
    "    weights = tf.Variable(tf.random_normal([1024, 10]))\n",
    "    biases = tf.Variable(tf.random_normal([10]))\n",
    "    y_results = tf.add(tf.matmul(x, weights), biases, name='y_results')\n",
    "    return y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model defined with tf.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn_tf_layers(input):\n",
    "    x = tf.reshape(x_ph, [-1, 28, 28, 1])\n",
    "    x = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu, name='conv1')\n",
    "    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2, name='conv1_max_pool')\n",
    "    \n",
    "    x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu, name='conv2')\n",
    "    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2, name='conv2_max_pool')\n",
    "    \n",
    "    x = tf.reshape(x, [-1, 7 * 7 * 64]) # flatten 7,7,64 tensor\n",
    "    \n",
    "    x = tf.layers.dense(inputs=x, units=1024, activation=tf.nn.relu, name='dense1')\n",
    "\n",
    "    x = tf.layers.dense(inputs=x, units=10)\n",
    "    \n",
    "    y_results = tf.identity(x, name='y_results') # renaming the last layer\n",
    "    return y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting model and testing that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x_ph = tf.placeholder(tf.float32, [None, 784], name='x_ph')\n",
    "# Switch between networks\n",
    "# y_results = model_fn_raw(x_ph)\n",
    "y_results = model_fn_tf_layers(x_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_targets_ph = tf.placeholder(tf.float32, [None, 10], name='y_targets_ph')\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_targets_ph, logits=y_results))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to define everything in the graph only once, so thats why none of the\n",
    "# accuracy definitions are not inside the function (different from pytorch)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_arrays_op = tf.equal(tf.argmax(y_results, 1), tf.argmax(y_targets_ph, 1))\n",
    "    correct_count_op = tf.reduce_sum(tf.cast(correct_arrays_op, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before using any graph you have to start the session and initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.04598422,  0.06740789, -0.04518105, -0.10845773, -0.09133087,\n",
       "          0.09140872, -0.02506002,  0.05030861,  0.00887649,  0.06929632]], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that graph works by running once\n",
    "sess.run([y_results], feed_dict={x_ph: np.expand_dims(mnist_test.images[0], 0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that accuracy operations had to be defined outside the function\n",
    "def test_model_accuracy(dataset, batch_size = 32):\n",
    "    correct_count, test_loss = 0, 0\n",
    "    for batch_idx in range(int(mnist_test.num_examples / batch_size)):\n",
    "        data, target = dataset.next_batch(batch_size)\n",
    "\n",
    "        loss, c_count = sess.run([loss_op, correct_count_op],\n",
    "                                 feed_dict={x_ph: data, y_targets_ph: target})\n",
    "        correct_count += c_count\n",
    "        test_loss += loss\n",
    "\n",
    "    print('Test set: Average loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "                test_loss/batch_idx, correct_count, dataset.num_examples,\n",
    "                100. * correct_count / dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a graph with random accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.310419, Accuracy: 910/10000 (9.10%)\n"
     ]
    }
   ],
   "source": [
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.310373, Accuracy: 914/10000 (9.14%)\n",
      "Test set: Average loss: 2.310361, Accuracy: 909/10000 (9.09%)\n",
      "Test set: Average loss: 2.310484, Accuracy: 909/10000 (9.09%)\n",
      "Test set: Average loss: 2.310406, Accuracy: 910/10000 (9.10%)\n",
      "462 ms ± 11.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3\n",
    "# See speed and that accuracy should stay about the same\n",
    "# (some minor variation is fine because batch size does not match dataset size)\n",
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [14272/55000 (26%)]\tLoss: 0.091736\n",
      "Epoch: 1 [28544/55000 (52%)]\tLoss: 0.022093\n",
      "Epoch: 1 [42816/55000 (78%)]\tLoss: 0.057304\n",
      "Test set: Average loss: 0.047339, Accuracy: 9819/10000 (98.19%)\n",
      "Epoch: 2 [14272/55000 (26%)]\tLoss: 0.009110\n",
      "Epoch: 2 [28544/55000 (52%)]\tLoss: 0.000727\n",
      "Epoch: 2 [42816/55000 (78%)]\tLoss: 0.051671\n",
      "Test set: Average loss: 0.025791, Accuracy: 9893/10000 (98.93%)\n",
      "Epoch: 3 [14272/55000 (26%)]\tLoss: 0.005190\n",
      "Epoch: 3 [28544/55000 (52%)]\tLoss: 0.001360\n",
      "Epoch: 3 [42816/55000 (78%)]\tLoss: 0.013744\n",
      "Test set: Average loss: 0.030859, Accuracy: 9886/10000 (98.86%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "log_interval_percentage = .26\n",
    "log_interval = int(mnist_train.num_examples / batch_size * log_interval_percentage)\n",
    "for epoch in range(3):\n",
    "    total_epochs += 1\n",
    "    for batch_idx in range(int(mnist_train.num_examples / batch_size)):\n",
    "        data, target = mnist_train.next_batch(batch_size)\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={x_ph: data, y_targets_ph: target})\n",
    "        \n",
    "        if (batch_idx % log_interval == 0) and (batch_idx > 0):\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch+1, batch_idx * batch_size, mnist_train.num_examples,\n",
    "                        100. * batch_idx * batch_size / mnist_train.num_examples, loss))\n",
    "    \n",
    "    test_model_accuracy(mnist_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.031063, Accuracy: 9886/10000 (98.86%)\n"
     ]
    }
   ],
   "source": [
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = './data/tf_model'\n",
    "import os\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/tf_model/tf_mnist_simple-3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = saver.save(sess, model_dir+'/tf_mnist_simple', global_step=total_epochs)\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset session for loading\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network graph was saved alongside variables so no need to define the network in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/tf_model/tf_mnist_simple-3\n"
     ]
    }
   ],
   "source": [
    "# saved_model = './data/tf_model/tf_mnist_simple-5'\n",
    "loader = tf.train.import_meta_graph(saved_model + '.meta')\n",
    "loader.restore(sess, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_x_ph = graph.get_tensor_by_name(\"x_ph:0\")  # given name + :0\n",
    "loaded_y_results = graph.get_tensor_by_name(\"y_results:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_y_softmax_results = tf.nn.softmax(loaded_y_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 (softmax: 0.9999191761016846, correct: True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABoJJREFUeJztnF1IlXccxz+/mlagvZhlspU62EU34eC0LuwiKGntphkl\nFowt1gvUZAsvLIlYRNHFtqCblcNeLowxmaCFMGItwghTS+Yy3Gq4MlzToDbXizl/uzjn8fh29NHn\n8e85p/8H5JzzvPyf3/n6Pf/n//J7/qKqWMwxbaoDeNWwghvGCm4YK7hhrOCGsYIbxgpuGE+Ci8i7\nItIqIndEZK9fQcUzMtGOj4hMB34FcoF2oB7YrKot/oUXf7zm4dx3gDuq+juAiHwLrAciCp6amqqZ\nmZkeLhmdtLW10dXVJW6O9SL468D9AZ/bgRVDDxKRHcAOgCVLltDQ0ODhktFJIBBwfeyk3zRVtVRV\nA6oaWLBgwWRfLurxIvgDYPGAz2+EtllGwYvg9cBbIpIlIolAAVDtT1jxy4TrcFXtFZFPgB+A6cAp\nVb3lW2RxipebJqpaA9T4FMsrge1pGsYKbhgruGE81eHxyJUrVwCoq6sDYNu2bQDMmzfPl/Ktww1j\nHT6ElJQUAPbt2zfotbe315fyrcMNYx0e4sWLFwDDBtd27tzp63Wsww0TFw5vaQkOwR88eBCAvLw8\nCgoKXJ378uVLAHbv3g3AmTNnAFi6dCkAhw4d8jNU63DTxLTDm5qaANi6dSsAzc3NACxfvtx1GX19\nfYPOdVi4cCEASUlJnuMciHW4YWLK4T09PUC4N7h27dpB+x1X5ubmui7z6dOnwPDWSU5ODgCJiYkT\nCzYC1uGGiQmHP3/+HKC/5XH+/HkARIIT5U6L4vLlywDMnz/fddmHDx8eVNaKFcF58JKSEo9Rj4x1\nuGGi2uH37wezMIqKigC4cOECAMuWLQNg1apVABw5cgSAWbNmuS67tbUVgGPHjgEwZ84cADZt2gTA\nzJkzvYQeEetww0SVw51e39WrV4HwSF19fT0QHtfYvn07ANnZ2WOW6dT/u3btAmDPnj1AuO52fhVO\nL7WwsNDjtxidqBL87t27ABQXFwPQ2NgIhDObnJ+/m6ZaeXk5EP4nXrt2DYDKykoAuru7AUhOTgYg\nPz/f+xdwga1SDBNVDq+uDuYRDe2EOFWK8/N3uuPTpkX2i5tjALZs2QJAWlraBCIeP9bhhokqh7e3\ntwPhbvWjR48AmDFjBhAeSLp37x4QzMaNxOzZswFIT08H4PTp0yMed+DAAa9hjwvrcMNElcOPHz8+\n6POzZ8+AcD3sOP3x48cAzJ07N2JZTnNww4YNg7Y7HZv9+/cD5upuB+tww0SVw4cSqas+mrMBLl26\n1N+Wv3nzJgDr1q0D4OTJk0C4jjfNmA4XkcUi8pOItIjILRH5NLQ9RUQuishvoVd/UpPiHDcO7wWK\nVPWGiCQDjSJyEfgI+FFVj4YeGdwLFE9eqGPj9CY3btzIkydPgPB0W0VFBTB5g1JuGdPhqtqhqjdC\n7/8BbhN8oGo9cDZ02Fng/ckKMp4YVx0uIpnA20AdkKaqHaFdfwJmb/cDuH79OgArV67s3+a0v6uq\nqoCpd7aD61aKiCQB3wOfqerfA/dp8OnaEZ+wFZEdItIgIg2dnZ2ego0HXDlcRBIIil2uqpWhzQ9F\nJF1VO0QkHfhrpHNVtRQoBQgEAr6u9+RMUDgjfc402aJFi/pHGk23s8fCTStFgDLgtqp+NWBXNfBh\n6P2HQJX/4cUfbhyeA3wANItIU2hbCXAU+E5EPgb+AMwMKAM1NcHnuJxJBWcMxqGioiLqnO0wpuCq\nWgtEeo58tb/hxD9R3dOMRFlZGTDc2Q4nTpwgKysLCKchZ2RkmAluDOxYimFi0uHnzp0DoLa2FgjP\nT65Zs6b/mISEBMD/VDWvWIcbJiYd7oyLr14de/ds63DDWMENYwU3jBXcMFZww0x43cIJXUykE/gX\n6DJ2Uf9JZXj8GarqagU1o4IDiEiDqrpfdy7K8Bq/rVIMYwU3zFQIXjoF1/QTT/Ebr8NfdWyVYhhj\ngsfiWuOjZJ19LiIPRKQp9Pee6zJNVCmxutZ4KBshfWDWGcGEp3ygW1W/GG+Zphzev9a4qvYAzlrj\nUc0oWWcTxpTgI6017ilw0wzJOgMoFJGfReTUeBJZ7U3TBSNknX0NvAlkAx3Al27LMiV4zK41PlLW\nmao+VNX/VLUP+IZglekKU4LH5FrjkbLOQjdThzzgF7dlGpnTjOG1xiNlnW0WkWyCCaxtgOu19mxP\n0zD2pmkYK7hhrOCGsYIbxgpuGCu4YazghrGCG+Z/ZN0D0gOZLQMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e5aa3ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img_idx = int(np.random.random()*100)\n",
    "result = sess.run([loaded_y_softmax_results], \n",
    "                  feed_dict={loaded_x_ph: np.expand_dims(mnist_test.images[test_img_idx], 0)})\n",
    "result_class = np.argmax(result)\n",
    "result_correct = result_class == np.argmax(mnist_test.labels[test_img_idx])\n",
    "print(\"{} (softmax: {}, correct: {})\".format(result_class, result[0][0][result_class], result_correct))\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(mnist_test.images[test_img_idx].reshape(28,28)).set_cmap('Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to use names for everything so debugging becomes easier. Default names are not really descriptive. Above we used e.g. `with tf.name_scope(\"accuracy\"):`\n",
    "\n",
    "Use tensorboard for better insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'init' type=NoOp>,\n",
       " <tf.Operation 'x_ph' type=Placeholder>,\n",
       " <tf.Operation 'Reshape/shape' type=Const>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'conv1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/read' type=Identity>,\n",
       " <tf.Operation 'conv1/convolution/Shape' type=Const>,\n",
       " <tf.Operation 'conv1/convolution/dilation_rate' type=Const>,\n",
       " <tf.Operation 'conv1/convolution' type=Conv2D>,\n",
       " <tf.Operation 'conv1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv1/Relu' type=Relu>,\n",
       " <tf.Operation 'conv1_max_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'conv2/kernel' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/read' type=Identity>,\n",
       " <tf.Operation 'conv2/convolution/Shape' type=Const>,\n",
       " <tf.Operation 'conv2/convolution/dilation_rate' type=Const>,\n",
       " <tf.Operation 'conv2/convolution' type=Conv2D>,\n",
       " <tf.Operation 'conv2/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv2/Relu' type=Relu>,\n",
       " <tf.Operation 'conv2_max_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'dense1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/read' type=Identity>,\n",
       " <tf.Operation 'dense1/MatMul' type=MatMul>,\n",
       " <tf.Operation 'dense1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'dense1/Relu' type=Relu>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'dense/kernel' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/read' type=Identity>,\n",
       " <tf.Operation 'dense/MatMul' type=MatMul>,\n",
       " <tf.Operation 'dense/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'y_results' type=Identity>,\n",
       " <tf.Operation 'y_targets_ph' type=Placeholder>,\n",
       " <tf.Operation 'Rank' type=Const>,\n",
       " <tf.Operation 'Shape' type=Shape>,\n",
       " <tf.Operation 'Rank_1' type=Const>,\n",
       " <tf.Operation 'Shape_1' type=Shape>,\n",
       " <tf.Operation 'Sub/y' type=Const>,\n",
       " <tf.Operation 'Sub' type=Sub>,\n",
       " <tf.Operation 'Slice/begin' type=Pack>,\n",
       " <tf.Operation 'Slice/size' type=Const>,\n",
       " <tf.Operation 'Slice' type=Slice>,\n",
       " <tf.Operation 'concat/values_0' type=Const>,\n",
       " <tf.Operation 'concat/axis' type=Const>,\n",
       " <tf.Operation 'concat' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'Rank_2' type=Const>,\n",
       " <tf.Operation 'Shape_2' type=Shape>,\n",
       " <tf.Operation 'Sub_1/y' type=Const>,\n",
       " <tf.Operation 'Sub_1' type=Sub>,\n",
       " <tf.Operation 'Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'Slice_1/size' type=Const>,\n",
       " <tf.Operation 'Slice_1' type=Slice>,\n",
       " <tf.Operation 'concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'concat_1/axis' type=Const>,\n",
       " <tf.Operation 'concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape_3' type=Reshape>,\n",
       " <tf.Operation 'SoftmaxCrossEntropyWithLogits' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'Sub_2/y' type=Const>,\n",
       " <tf.Operation 'Sub_2' type=Sub>,\n",
       " <tf.Operation 'Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'Slice_2' type=Slice>,\n",
       " <tf.Operation 'Reshape_4' type=Reshape>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Mean' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/Mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/Mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/Mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/Reshape_4_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_4_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/conv2_max_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv2/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1_max_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv1/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'beta1_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta1_power' type=VariableV2>,\n",
       " <tf.Operation 'beta1_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta1_power/read' type=Identity>,\n",
       " <tf.Operation 'beta2_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta2_power' type=VariableV2>,\n",
       " <tf.Operation 'beta2_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta2_power/read' type=Identity>,\n",
       " <tf.Operation 'conv1/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv2/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense1/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Adam/learning_rate' type=Const>,\n",
       " <tf.Operation 'Adam/beta1' type=Const>,\n",
       " <tf.Operation 'Adam/beta2' type=Const>,\n",
       " <tf.Operation 'Adam/epsilon' type=Const>,\n",
       " <tf.Operation 'Adam/update_conv1/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv1/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv2/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv2/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense1/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense1/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Operation 'accuracy/ArgMax/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/ArgMax_1/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax_1' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/Cast' type=Cast>,\n",
       " <tf.Operation 'accuracy/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/Sum' type=Sum>,\n",
       " <tf.Operation 'init_1' type=NoOp>,\n",
       " <tf.Operation 'save/Const' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/SaveV2' type=SaveV2>,\n",
       " <tf.Operation 'save/control_dependency' type=Identity>,\n",
       " <tf.Operation 'save/RestoreV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_1/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_1' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_2' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_3/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_3' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_4/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_4' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_5/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_5' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_6/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_6' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_7/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_7' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_8/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_8' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_9/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_9' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_10/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_10' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_11/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_11' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_12/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_12' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_13/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_13' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_14/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_14' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_15/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_15' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_16/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_16' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_17/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_17' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_18/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_18' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_19/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_19' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_20/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_20' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_21/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_21' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_22/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_22' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_23/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_23' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_24/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_24' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_25/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_25' type=Assign>,\n",
       " <tf.Operation 'save/restore_all' type=NoOp>,\n",
       " <tf.Operation 'Softmax' type=Softmax>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/kernel/Adam:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/kernel/Adam_1:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias/Adam:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias/Adam_1:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel/Adam:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel/Adam_1:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias/Adam:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias/Adam_1:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel/Adam:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel/Adam_1:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias/Adam:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias/Adam_1:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adam:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adam_1:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change the graph its often nice to reset either the whole notebook or at least reset the graph with the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_def]",
   "language": "python",
   "name": "conda-env-py3_def-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
