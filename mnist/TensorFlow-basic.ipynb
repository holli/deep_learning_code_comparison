{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/tf_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/tf_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/tf_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/tf_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_input_data = input_data.read_data_sets('./data/tf_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len 55000, test_len 10000\n"
     ]
    }
   ],
   "source": [
    "mnist_train = mnist_input_data.train\n",
    "mnist_test = mnist_input_data.test\n",
    "print(\"train_len {}, test_len {}\".format(mnist_train.num_examples, mnist_test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample class: [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19c32dd588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample class: {}\".format(mnist_train.labels[0]))\n",
    "sample_img = mnist_train.images[0]\n",
    "sample_img = sample_img.reshape(28,28)\n",
    "# print(\"Sample size: {}\".format(mnist_train.images[0].size()))\n",
    "plt.imshow(sample_img).set_cmap('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are two different network definitions. Half of the tutorials seem to do everything manually and half use tf.layers so they are both shown here. There are no big functional differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw model by manually defining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn_raw(x_ph):\n",
    "    x = tf.reshape(x_ph, [-1, 28, 28, 1])\n",
    "\n",
    "    with tf.name_scope('conv1'):\n",
    "        weights = tf.Variable(tf.random_normal([5, 5, 1, 32]))\n",
    "        biases = tf.Variable(tf.random_normal([32]))\n",
    "        x = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        weights = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "        biases = tf.Variable(tf.random_normal([64]))\n",
    "        x = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.name_scope('dense1'):\n",
    "        weights = tf.Variable(tf.random_normal([7*7*64, 1024]))\n",
    "        biases = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "        x = tf.reshape(x, [-1, weights.get_shape().as_list()[0]])\n",
    "        x = tf.add(tf.matmul(x, weights), biases)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # This would be one way to use dropout. In sess.run we would have to add\n",
    "        # {dropout_keep_prob: 0.5} to feed_dict while training\n",
    "        # dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        # x = tf.nn.dropout(x, dropout_keep_prob)\n",
    "\n",
    "    weights = tf.Variable(tf.random_normal([1024, 10]))\n",
    "    biases = tf.Variable(tf.random_normal([10]))\n",
    "    y_results = tf.add(tf.matmul(x, weights), biases, name='y_results')\n",
    "    return y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model defined with tf.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn_tf_layers(input):\n",
    "    x = tf.reshape(x_ph, [-1, 28, 28, 1])\n",
    "    x = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu, name='conv1')\n",
    "    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2, name='conv1_max_pool')\n",
    "    \n",
    "    x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu, name='conv2')\n",
    "    x = tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2, name='conv2_max_pool')\n",
    "    \n",
    "    x = tf.reshape(x, [-1, 7 * 7 * 64]) # flatten 7,7,64 tensor\n",
    "    \n",
    "    x = tf.layers.dense(inputs=x, units=1024, activation=tf.nn.relu, name='dense1')\n",
    "\n",
    "    x = tf.layers.dense(inputs=x, units=10)\n",
    "    \n",
    "    y_results = tf.identity(x, name='y_results') # renaming the last layer\n",
    "    return y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting model and testing that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x_ph = tf.placeholder(tf.float32, [None, 784], name='x_ph')\n",
    "# Switch between networks\n",
    "# y_results = model_fn_raw(x_ph)\n",
    "y_results = model_fn_tf_layers(x_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets_ph = tf.placeholder(tf.float32, [None, 10], name='y_targets_ph')\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_targets_ph, logits=y_results))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to define everything in the graph only once, so thats why none of the\n",
    "# accuracy definitions are not inside the function (different from pytorch)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_arrays_op = tf.equal(tf.argmax(y_results, 1), tf.argmax(y_targets_ph, 1))\n",
    "    correct_count_op = tf.reduce_sum(tf.cast(correct_arrays_op, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before using any graph you have to start the session and initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0027089 , -0.09174412,  0.01719421,  0.01585919, -0.00880219,\n",
       "          0.03387811, -0.0161695 ,  0.0635296 ,  0.03651213,  0.03110645]], dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that graph works by one prediction\n",
    "sess.run([y_results], feed_dict={x_ph: np.expand_dims(mnist_test.images[0], 0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that accuracy operations had to be defined outside the function\n",
    "def test_model_accuracy(dataset, batch_size = 32):\n",
    "    correct_count, test_loss = 0, 0\n",
    "    for batch_idx in range(int(mnist_test.num_examples / batch_size)):\n",
    "        data, target = dataset.next_batch(batch_size)\n",
    "\n",
    "        loss, c_count = sess.run([loss_op, correct_count_op],\n",
    "                                 feed_dict={x_ph: data, y_targets_ph: target})\n",
    "        correct_count += c_count\n",
    "        test_loss += loss\n",
    "\n",
    "    print('Test set: Average loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "                test_loss/batch_idx, correct_count, dataset.num_examples,\n",
    "                100. * correct_count / dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a graph with random accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.307633, Accuracy: 933/10000 (9.33%)\n"
     ]
    }
   ],
   "source": [
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.307578, Accuracy: 940/10000 (9.40%)\n",
      "Test set: Average loss: 2.307662, Accuracy: 917/10000 (9.17%)\n",
      "Test set: Average loss: 2.307646, Accuracy: 935/10000 (9.35%)\n",
      "Test set: Average loss: 2.307759, Accuracy: 922/10000 (9.22%)\n",
      "442 ms ± 11.2 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3\n",
    "# See speed and that accuracy should stay about the same\n",
    "# (some minor variation is fine because batch size does not match dataset size)\n",
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [14272/55000 (26%)]\tLoss: 0.132131\n",
      "Epoch: 1 [28544/55000 (52%)]\tLoss: 0.082277\n",
      "Epoch: 1 [42816/55000 (78%)]\tLoss: 0.003758\n",
      "Test set: Average loss: 0.039138, Accuracy: 9850/10000 (98.50%)\n",
      "Epoch: 2 [14272/55000 (26%)]\tLoss: 0.011728\n",
      "Epoch: 2 [28544/55000 (52%)]\tLoss: 0.002272\n",
      "Epoch: 2 [42816/55000 (78%)]\tLoss: 0.063499\n",
      "Test set: Average loss: 0.035120, Accuracy: 9872/10000 (98.72%)\n",
      "Epoch: 3 [14272/55000 (26%)]\tLoss: 0.000360\n",
      "Epoch: 3 [28544/55000 (52%)]\tLoss: 0.000280\n",
      "Epoch: 3 [42816/55000 (78%)]\tLoss: 0.000253\n",
      "Test set: Average loss: 0.023795, Accuracy: 9913/10000 (99.13%)\n",
      "Epoch: 4 [14272/55000 (26%)]\tLoss: 0.007457\n",
      "Epoch: 4 [28544/55000 (52%)]\tLoss: 0.015664\n",
      "Epoch: 4 [42816/55000 (78%)]\tLoss: 0.011706\n",
      "Test set: Average loss: 0.034446, Accuracy: 9887/10000 (98.87%)\n",
      "Epoch: 5 [14272/55000 (26%)]\tLoss: 0.003536\n",
      "Epoch: 5 [28544/55000 (52%)]\tLoss: 0.007217\n",
      "Epoch: 5 [42816/55000 (78%)]\tLoss: 0.000499\n",
      "Test set: Average loss: 0.045338, Accuracy: 9870/10000 (98.70%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "log_interval_percentage = .26\n",
    "log_interval = int(mnist_train.num_examples / batch_size * log_interval_percentage)\n",
    "for epoch in range(5):\n",
    "    total_epochs += 1\n",
    "    for batch_idx in range(int(mnist_train.num_examples / batch_size)):\n",
    "        data, target = mnist_train.next_batch(batch_size)\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={x_ph: data, y_targets_ph: target})\n",
    "        \n",
    "        if (batch_idx % log_interval == 0) and (batch_idx > 0):\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch+1, batch_idx * batch_size, mnist_train.num_examples,\n",
    "                        100. * batch_idx * batch_size / mnist_train.num_examples, loss))\n",
    "    \n",
    "    test_model_accuracy(mnist_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.047651, Accuracy: 9868/10000 (98.68%)\n"
     ]
    }
   ],
   "source": [
    "test_model_accuracy(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = './data/tf_model'\n",
    "import os\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/tf_model/tf_mnist_simple-5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = saver.save(sess, model_dir+'/tf_mnist_simple', global_step=total_epochs)\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset session for loading\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network graph was saved alongside variables so no need to define the network in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/tf_model/tf_mnist_simple-5\n"
     ]
    }
   ],
   "source": [
    "# saved_model = './data/tf_model/tf_mnist_simple-5'\n",
    "loader = tf.train.import_meta_graph(saved_model + '.meta')\n",
    "loader.restore(sess, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_x_ph = graph.get_tensor_by_name(\"x_ph:0\")  # given name + :0\n",
    "loaded_y_results = graph.get_tensor_by_name(\"y_results:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 : 9.119844436645508, (correct : True)\n"
     ]
    }
   ],
   "source": [
    "# Predicting with loaded network\n",
    "test_img_idx = int(np.random.random()*100)\n",
    "result = sess.run([loaded_y_results], \n",
    "                  feed_dict={loaded_x_ph: np.expand_dims(mnist_test.images[test_img_idx], 0)})\n",
    "result_class = np.argmax(result)\n",
    "result_correct = result_class == np.argmax(mnist_test.labels[test_img_idx])\n",
    "print(\"{} : {}, (correct : {})\".format(result_class, result[0][0][result_class], result_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to use names for everything so debugging becomes easier. Default names are not really descriptive. Above we used e.g. `with tf.name_scope(\"accuracy\"):`\n",
    "\n",
    "Use tensorboard for better insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'init' type=NoOp>,\n",
       " <tf.Operation 'x_ph' type=Placeholder>,\n",
       " <tf.Operation 'Reshape/shape' type=Const>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'conv1/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'conv1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/read' type=Identity>,\n",
       " <tf.Operation 'conv1/convolution/Shape' type=Const>,\n",
       " <tf.Operation 'conv1/convolution/dilation_rate' type=Const>,\n",
       " <tf.Operation 'conv1/convolution' type=Conv2D>,\n",
       " <tf.Operation 'conv1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv1/Relu' type=Relu>,\n",
       " <tf.Operation 'conv1_max_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'conv2/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'conv2/kernel' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/read' type=Identity>,\n",
       " <tf.Operation 'conv2/convolution/Shape' type=Const>,\n",
       " <tf.Operation 'conv2/convolution/dilation_rate' type=Const>,\n",
       " <tf.Operation 'conv2/convolution' type=Conv2D>,\n",
       " <tf.Operation 'conv2/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv2/Relu' type=Relu>,\n",
       " <tf.Operation 'conv2_max_pool/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dense1/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'dense1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/read' type=Identity>,\n",
       " <tf.Operation 'dense1/MatMul' type=MatMul>,\n",
       " <tf.Operation 'dense1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'dense1/Relu' type=Relu>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dense/kernel/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'dense/kernel' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/read' type=Identity>,\n",
       " <tf.Operation 'dense/MatMul' type=MatMul>,\n",
       " <tf.Operation 'dense/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'y_results' type=Identity>,\n",
       " <tf.Operation 'y_targets_ph' type=Placeholder>,\n",
       " <tf.Operation 'Rank' type=Const>,\n",
       " <tf.Operation 'Shape' type=Shape>,\n",
       " <tf.Operation 'Rank_1' type=Const>,\n",
       " <tf.Operation 'Shape_1' type=Shape>,\n",
       " <tf.Operation 'Sub/y' type=Const>,\n",
       " <tf.Operation 'Sub' type=Sub>,\n",
       " <tf.Operation 'Slice/begin' type=Pack>,\n",
       " <tf.Operation 'Slice/size' type=Const>,\n",
       " <tf.Operation 'Slice' type=Slice>,\n",
       " <tf.Operation 'concat/values_0' type=Const>,\n",
       " <tf.Operation 'concat/axis' type=Const>,\n",
       " <tf.Operation 'concat' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'Rank_2' type=Const>,\n",
       " <tf.Operation 'Shape_2' type=Shape>,\n",
       " <tf.Operation 'Sub_1/y' type=Const>,\n",
       " <tf.Operation 'Sub_1' type=Sub>,\n",
       " <tf.Operation 'Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'Slice_1/size' type=Const>,\n",
       " <tf.Operation 'Slice_1' type=Slice>,\n",
       " <tf.Operation 'concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'concat_1/axis' type=Const>,\n",
       " <tf.Operation 'concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape_3' type=Reshape>,\n",
       " <tf.Operation 'SoftmaxCrossEntropyWithLogits' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'Sub_2/y' type=Const>,\n",
       " <tf.Operation 'Sub_2' type=Sub>,\n",
       " <tf.Operation 'Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'Slice_2' type=Slice>,\n",
       " <tf.Operation 'Reshape_4' type=Reshape>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Mean' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/Mean_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/Mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/Mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/Mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/Mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/Reshape_4_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_4_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/SoftmaxCrossEntropyWithLogits_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dense1/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/conv2_max_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv2/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv2/convolution_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1_max_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv1/Relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv1/convolution_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'beta1_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta1_power' type=VariableV2>,\n",
       " <tf.Operation 'beta1_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta1_power/read' type=Identity>,\n",
       " <tf.Operation 'beta2_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta2_power' type=VariableV2>,\n",
       " <tf.Operation 'beta2_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta2_power/read' type=Identity>,\n",
       " <tf.Operation 'conv1/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv1/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv1/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv2/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'conv2/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv2/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense1/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense1/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense1/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense/kernel/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/kernel/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/kernel/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Adam/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias/Adam' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/Adam/read' type=Identity>,\n",
       " <tf.Operation 'dense/bias/Adam_1/Initializer/Const' type=Const>,\n",
       " <tf.Operation 'dense/bias/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'dense/bias/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'dense/bias/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Adam/learning_rate' type=Const>,\n",
       " <tf.Operation 'Adam/beta1' type=Const>,\n",
       " <tf.Operation 'Adam/beta2' type=Const>,\n",
       " <tf.Operation 'Adam/epsilon' type=Const>,\n",
       " <tf.Operation 'Adam/update_conv1/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv1/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv2/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv2/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense1/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense1/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense/kernel/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_dense/bias/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam' type=NoOp>,\n",
       " <tf.Operation 'accuracy/ArgMax/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/ArgMax_1/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax_1' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/Cast' type=Cast>,\n",
       " <tf.Operation 'accuracy/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/Sum' type=Sum>,\n",
       " <tf.Operation 'init_1' type=NoOp>,\n",
       " <tf.Operation 'save/Const' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/SaveV2' type=SaveV2>,\n",
       " <tf.Operation 'save/control_dependency' type=Identity>,\n",
       " <tf.Operation 'save/RestoreV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_1/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_1' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_2' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_3/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_3' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_4/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_4' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_5/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_5' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_6/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_6' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_7/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_7' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_8/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_8' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_9/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_9' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_10/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_10' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_11/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_11' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_12/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_12' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_13/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_13' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_14/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_14' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_15/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_15' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_16/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_16' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_17/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_17' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_18/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_18' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_19/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_19' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_20/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_20' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_21/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_21' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_22/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_22' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_23/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_23' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_24/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_24' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_25/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_25' type=Assign>,\n",
       " <tf.Operation 'save/restore_all' type=NoOp>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/kernel/Adam:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/kernel/Adam_1:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias/Adam:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias/Adam_1:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel/Adam:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel/Adam_1:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias/Adam:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias/Adam_1:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel/Adam:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/kernel/Adam_1:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias/Adam:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense1/bias/Adam_1:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adam:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adam_1:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change the graph its often nice to reset either the whole notebook or at least reset the graph with the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_def]",
   "language": "python",
   "name": "conda-env-py3_def-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
